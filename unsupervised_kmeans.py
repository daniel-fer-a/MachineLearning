# -*- coding: utf-8 -*-
"""unsupervised_kmeans

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iPVr2DUb21r74sswB24N8Vx3jWeVpaH0
"""

# unsupervised_kmeans.py
# Análisis no supervisado (K-Means) sobre Home Credit - EA3

import pandas as pd
import numpy as np

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ============================================================
# 0. CONFIGURACIÓN BÁSICA
# ============================================================

# Ruta al conjunto de entrenamiento (SOLO TRAIN, nada de test/validación)
PATH_APPLICATION_TRAIN = "data/application_train.parquet"  # ajusta ruta/nombre

# Nombre de la variable objetivo (para analizar riesgo por cluster, no para entrenar)
TARGET_COL = "TARGET"

# (Opcional) nombre de la columna con la PD del modelo supervisado sobre TRAIN
# Si aún no tienes la PD guardada en una columna, déjalo como None.
PD_COL = None     # por ejemplo: "PD_MODEL" o "SCORE_SUPERVISED"

# Número de clusters final elegido (luego de revisar la tabla de calidad)
K_FINAL = 4       # puedes cambiarlo tras ver los resultados de K


# ============================================================
# 1. CARGA DE DATOS (CRISP-DM: Entendimiento de datos)
# ============================================================

print("Cargando dataset de entrenamiento...")
df = pd.read_parquet(PATH_APPLICATION_TRAIN)

print("Shape original:", df.shape)
print("Columnas disponibles:", len(df.columns))

if TARGET_COL not in df.columns:
    raise ValueError(
        f"No se encontró la columna TARGET '{TARGET_COL}' en el dataset. "
        "Asegúrate de que el archivo de entrenamiento contiene la variable objetivo."
    )

y = df[TARGET_COL]

if PD_COL is not None and PD_COL not in df.columns:
    raise ValueError(
        f"Definiste PD_COL='{PD_COL}', pero esa columna no existe en df. "
        "Cámbiala o pon PD_COL = None."
    )

# ============================================================
# 2. SELECCIÓN DE VARIABLES PARA CLUSTERING
#    (CRISP-DM: Preparación de datos)
# ============================================================

# Importante: NO usamos TARGET ni ninguna info de valid/test.
# Usamos solo variables de perfil del cliente en application_train.

numeric_features = [
    # Ajusta a tu gusto; estas son columnas típicas de Home Credit:
    "AMT_INCOME_TOTAL",
    "AMT_CREDIT",
    "AMT_ANNUITY",
    "AMT_GOODS_PRICE",
    "CNT_CHILDREN",
    "DAYS_BIRTH",
    "DAYS_EMPLOYED",
    "DAYS_REGISTRATION",
    "DAYS_ID_PUBLISH",
    "DAYS_LAST_PHONE_CHANGE",
    "EXT_SOURCE_1",
    "EXT_SOURCE_2",
    "EXT_SOURCE_3",
    "REGION_POPULATION_RELATIVE",
]

categorical_features = [
    "NAME_CONTRACT_TYPE",
    "CODE_GENDER",
    "FLAG_OWN_CAR",
    "FLAG_OWN_REALTY",
    "NAME_INCOME_TYPE",
    "NAME_EDUCATION_TYPE",
    "NAME_FAMILY_STATUS",
    "NAME_HOUSING_TYPE",
    "ORGANIZATION_TYPE",
]

# Filtramos solo las columnas que realmente existen en df
numeric_features = [c for c in numeric_features if c in df.columns]
categorical_features = [c for c in categorical_features if c in df.columns]

print("\nVariables numéricas usadas para clustering:", numeric_features)
print("Variables categóricas usadas para clustering:", categorical_features)

X = df[numeric_features + categorical_features].copy()


# ============================================================
# 3. PIPELINE DE PREPROCESAMIENTO
#    (imputación + escalado + one-hot)
# ============================================================

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ]
)

# ============================================================
# 4. BÚSQUEDA DEL NÚMERO DE CLUSTERS (K)
#    (CRISP-DM: Modelado)
# ============================================================

k_values = [2, 3, 4, 5, 6, 7, 8]
results = []

print("\nBuscando K óptimo (inertia + silhouette):")
for k in k_values:
    model_k = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("cluster", KMeans(n_clusters=k, random_state=42, n_init="auto"))
    ])
    model_k.fit(X)

    # Inercia (suma de distancias intra-cluster)
    inertia = model_k.named_steps["cluster"].inertia_

    # Para silueta, necesitamos los datos transformados
    X_transformed = model_k.named_steps["preprocess"].transform(X)
    labels = model_k.named_steps["cluster"].labels_
    sil = silhouette_score(X_transformed, labels)

    results.append({"k": k, "inertia": inertia, "silhouette": sil})
    print(f"K={k}: inertia={inertia:.2f}, silhouette={sil:.4f}")

results_df = pd.DataFrame(results)
results_df.to_csv("kmeans_k_search_results.csv", index=False)
print("\nTabla con resultados guardada en 'kmeans_k_search_results.csv'.")


# ============================================================
# 5. ENTRENAMIENTO DEL MODELO FINAL CON K_FINAL
# ============================================================

print(f"\nEntrenando modelo final con K={K_FINAL}...")
final_model = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("cluster", KMeans(n_clusters=K_FINAL, random_state=42, n_init="auto"))
])

final_model.fit(X)

# Asignar labels
X_transformed_final = final_model.named_steps["preprocess"].transform(X)
cluster_labels = final_model.named_steps["cluster"].labels_
df["CLUSTER_KMEANS"] = cluster_labels

print("Clusters asignados. Ejemplo de distribución:")
print(df["CLUSTER_KMEANS"].value_counts(normalize=True).sort_index())


# ============================================================
# 6. ANÁLISIS DE RIESGO POR CLUSTER
#    (CRISP-DM: Evaluación / Interpretación)
# ============================================================

grouped = df.groupby("CLUSTER_KMEANS")

cluster_summary = grouped.agg(
    n_clientes=("CLUSTER_KMEANS", "count"),
    tasa_mora=(TARGET_COL, "mean"),
)

if PD_COL is not None:
    cluster_summary["pd_media"] = grouped[PD_COL].mean()

cluster_summary["porcentaje"] = cluster_summary["n_clientes"] / len(df)

print("\nResumen de riesgo por cluster:")
print(cluster_summary)

cluster_summary.to_csv("kmeans_cluster_summary.csv")

# Ejemplo de perfil promedio de algunas variables numéricas
profile_numeric = grouped[numeric_features].mean()
profile_numeric.to_csv("kmeans_cluster_profile_numeric.csv")

print("\nPerfil promedio de variables numéricas por cluster guardado en 'kmeans_cluster_profile_numeric.csv'.")

# ============================================================
# 7. GUARDAR DATASET ENRIQUECIDO
# ============================================================

OUTPUT_PATH = "application_train_with_clusters.parquet"
df.to_parquet(OUTPUT_PATH)
print(f"\nDataset con columna CLUSTER_KMEANS guardado en: {OUTPUT_PATH}")

print("\n=== FIN DEL SCRIPT DE K-MEANS (EA3) ===")